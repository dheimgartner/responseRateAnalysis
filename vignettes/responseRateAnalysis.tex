\documentclass[a4paper]{article}\usepackage[]{graphicx}\usepackage[]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlsng}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hldef}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

\usepackage{Rd}

% kableExtra
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage[utf8]{inputenc}
\usepackage{makecell}
\usepackage{xcolor}

% \VignetteIndexEntry{responseRateAnalysis}
% \VignettePackage{responseRateAnalysis}

% Definitions
\newcommand{\slan}{{\sffamily S}}
\newcommand{\rlan}{{\sffamily R}}
\newcommand{\grid}{\pkg{grid}}
\newcommand{\responseRateAnalysis}{\pkg{responseRateAnalysis}}
\newcommand{\lattice}{\CRANpkg{lattice}}

\setlength{\parindent}{0in}
\setlength{\parskip}{.1in}
\setlength{\textwidth}{140mm}
\setlength{\oddsidemargin}{10mm}

\title{\responseRateAnalysis{} - Overview}
\author{Daniel Heimgartner}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\maketitle






The default data can be loaded with \code{default\_data()}. This applies the logistic transformation to the response rate, divides the response burden score by 1000 and defines the weights to be \code{sqrt(sample\_size)}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> dat <- default_data()
> head(dat)
#> # A tibble: 6 x 13
#>        y     x weight  year authors  survey_id response_rate
#>    <dbl> <dbl>  <dbl> <dbl> <chr>        <dbl>         <dbl>
#> 1  0.741 0.12    39.5  2004 Vrtic a~         1          67.7
#> 2  0.889 0.12    35.1  2006 Vrtic a~         2          70.9
#> 3  0.110 0.303   48.1  2007 Axhause~         3          52.7
#> 4 -0.584 0.44    22.4  2004 Locatel~         4          35.8
#> 5 -0.778 0.526   43.6  2006 JÃ¤ggle           5          31.5
#> 6 -0.572 0.231   96.6  2005 Waldner~         6          36.1
#> # i 6 more variables: response_burden_score <dbl>,
#> #   sample_size <dbl>, yes_yes <dbl>, yes_no <dbl>,
#> #   no_no <dbl>, no_yes <dbl>
\end{verbatim}
\end{kframe}
\end{knitrout}

The model is a simple weighted linear regression model with a logistic transformation of the response. We can add clustered standard errors and p-values with the \code{add\_clustered()}. The \code{summary()} generic has a method for class "clustered" (using \code{texreg::screenreg()} under the hood).

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> fit <- lm(y ~ 0 + x + yes_yes + yes_no + no_no + no_yes,
+           data = dat, weights = weight)
> m1 <- add_clustered(fit, cluster = dat$survey_id, type = "CR2")
> summary(m1)  # texreg::screenreg(fit_)
#> 
#> ======================
#>            Model 1    
#> ----------------------
#> x            -1.01 ***
#>              (0.19)   
#> yes_yes       1.89 ***
#>              (0.25)   
#> yes_no        0.63    
#>              (0.33)   
#> no_no        -0.64    
#>              (0.34)   
#> no_yes       -0.73    
#>              (0.57)   
#> ----------------------
#> R^2           0.81    
#> Adj. R^2      0.80    
#> Num. obs.    80       
#> LL         -106.09    
#> AIC         224.19    
#> BIC         238.48    
#> ======================
#> *** p < 0.001; ** p < 0.01; * p < 0.05
\end{verbatim}
\end{kframe}
\end{knitrout}

Let's estimate separate models for the different categories (recruitment x incentive).

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> dat_yes_yes <- subset(dat, yes_yes == 1)
> fit <- lm(formula = y ~ x,
+           data = dat_yes_yes, weights = weight)
> m2 <- add_clustered(fit, cluster = dat_yes_yes$survey_id, type = "CR2")
> 
> ## helper
> estimator <- function(dat, subset) {
+   dat_ <- subset(dat, subset = subset)
+   fit <- lm(formula = y ~ x,
+             data = dat_, weights = weight)
+   m <- add_clustered(fit, cluster = dat_$survey_id, type = "CR2")
+   m
+ }
> 
> m3 <- estimator(dat, subset = (dat$yes_no == 1))
> m4 <- estimator(dat, subset = (dat$no_no == 1))
> m5 <- estimator(dat, subset = (dat$no_yes == 1))
\end{verbatim}
\end{kframe}
\end{knitrout}

Let's compare the results to the findings from the last publication. We essentially repeat the above steps but for a subset of the data.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> dat_last <- subset(dat[1:67, ], sample_size >= 10)
> fit <- lm(y ~ 0 + x + yes_yes + yes_no + no_no + no_yes,
+           data = dat_last, weights = weight)
> m1_last <- add_clustered(fit, cluster = dat_last$survey_id, type = "CR2")
> m2_last <- estimator(dat_last, subset = (dat_last$yes_yes == 1))
> m3_last <- estimator(dat_last, subset = (dat_last$yes_no == 1))
> m4_last <- estimator(dat_last, subset = (dat_last$no_no == 1))
> # m5_last (no no_yes combinations at that time)
\end{verbatim}
\end{kframe}
\end{knitrout}

We can easily create a regression table with \pkg{texreg}.

\begin{kframe}
\begin{verbatim}
> m_last <- list(Pooled = m1_last,
+                `Yes, yes` = m2_last,
+                `Yes, no`= m3_last,
+                `No, no` = m4_last)
> texreg::texreg(m_last, caption = "Old models")
\end{verbatim}
\end{kframe}
\begin{table}
\begin{center}
\begin{tabular}{l c c c c}
\hline
 & Pooled & Yes, yes & Yes, no & No, no \\
\hline
x           & $-0.58^{***}$ & $-0.32^{**}$ & $-1.55$     & $-1.25^{***}$ \\
            & $(0.17)$      & $(0.11)$     & $(1.16)$    & $(0.31)$      \\
yes\_yes    & $1.53^{***}$  &              &             &               \\
            & $(0.20)$      &              &             &               \\
yes\_no     & $0.84^{***}$  &              &             &               \\
            & $(0.15)$      &              &             &               \\
no\_no      & $-1.11^{***}$ &              &             &               \\
            & $(0.20)$      &              &             &               \\
(Intercept) &               & $1.28^{***}$ & $1.13^{**}$ & $-0.81^{***}$ \\
            &               & $(0.18)$     & $(0.32)$    & $(0.20)$      \\
\hline
R$^2$       & $0.79$        & $0.13$       & $0.13$      & $0.22$        \\
Adj. R$^2$  & $0.78$        & $0.07$       & $0.07$      & $0.19$        \\
Num. obs.   & $66$          & $18$         & $18$        & $30$          \\
LL          & $-69.99$      & $-20.08$     & $-13.94$    & $-31.10$      \\
AIC         & $149.98$      & $46.16$      & $33.89$     & $68.20$       \\
BIC         & $160.93$      & $48.83$      & $36.56$     & $72.40$       \\
\hline
\multicolumn{5}{l}{\scriptsize{$^{***}p<0.001$; $^{**}p<0.01$; $^{*}p<0.05$}}
\end{tabular}
\caption{Old models}
\label{table:coefficients}
\end{center}
\end{table}


\begin{kframe}
\begin{verbatim}
> m <- list(Pooled = m1,
+           `Yes, yes` = m2,
+           `Yes, no` = m3,
+           `No, no` = m4,
+           `No, yes` = m5)
> texreg::texreg(m, caption = "New models")
\end{verbatim}
\end{kframe}
\begin{table}
\begin{center}
\begin{tabular}{l c c c c c}
\hline
 & Pooled & Yes, yes & Yes, no & No, no & No, yes \\
\hline
x           & $-1.01^{***}$ & $-0.31^{**}$ & $-2.98^{***}$ & $-1.63^{**}$ & $-1.05$  \\
            & $(0.19)$      & $(0.11)$     & $(0.71)$      & $(0.47)$     & $(0.40)$ \\
yes\_yes    & $1.89^{***}$  &              &               &              &          \\
            & $(0.25)$      &              &               &              &          \\
yes\_no     & $0.63$        &              &               &              &          \\
            & $(0.33)$      &              &               &              &          \\
no\_no      & $-0.64$       &              &               &              &          \\
            & $(0.34)$      &              &               &              &          \\
no\_yes     & $-0.73$       &              &               &              &          \\
            & $(0.57)$      &              &               &              &          \\
(Intercept) &               & $1.23^{***}$ & $1.49^{***}$  & $-0.38$      & $-0.64$  \\
            &               & $(0.16)$     & $(0.25)$      & $(0.41)$     & $(0.74)$ \\
\hline
R$^2$       & $0.81$        & $0.13$       & $0.72$        & $0.16$       & $0.92$   \\
Adj. R$^2$  & $0.80$        & $0.08$       & $0.70$        & $0.13$       & $0.90$   \\
Num. obs.   & $80$          & $20$         & $21$          & $34$         & $5$      \\
LL          & $-106.09$     & $-20.97$     & $-17.01$      & $-47.70$     & $-2.48$  \\
AIC         & $224.19$      & $47.94$      & $40.02$       & $101.39$     & $10.97$  \\
BIC         & $238.48$      & $50.92$      & $43.16$       & $105.97$     & $9.80$   \\
\hline
\multicolumn{6}{l}{\scriptsize{$^{***}p<0.001$; $^{**}p<0.01$; $^{*}p<0.05$}}
\end{tabular}
\caption{New models}
\label{table:coefficients}
\end{center}
\end{table}


Finally, we want to visualize our sample along with the model implied response rate curves.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> new_x <- seq(min(dat$x), max(dat$x), by = 0.01)
> newdata <- data.frame(x = new_x)
> 
> m_ <- m[2:length(m)]
> p <-
+   map2(m_, names(m_), function(x, y) {
+     p <- as.data.frame(predict(x, newdata = newdata, interval = "confidence"))
+     p$name <- y
+     p
+   })
> df <- reduce(p, rbind)
> 
> ## backtransform (undo the logit)
> df <-
+   df %>%
+   mutate(across(c(fit, lwr, upr), function(x) backtransform(x)),
+          x = rep(1000 * newdata$x, 4))
> 
> survey_data <-
+   dat %>%
+   mutate(yb = backtransform(y),
+          xb = x * 1000) %>%
+   pivot_longer(c(yes_yes, yes_no, no_no, no_yes)) %>%
+   mutate(name = case_when(name == "yes_yes" ~ "Yes, yes",
+                           name == "yes_no" ~ "Yes, no",
+                           name == "no_no" ~ "No, no",
+                           name == "no_yes" ~ "No, yes")) %>%
+   filter(value == 1)
> 
> option <- "C"
> df %>%
+   ggplot(aes(x = x, group = name)) +
+   geom_point(aes(x = xb, y = yb, col = name), data = survey_data) +
+   geom_line(aes(y = fit, col = name)) +
+   geom_ribbon(aes(ymin = lwr, ymax = upr, fill = name), alpha = 0.2) +
+   scale_fill_viridis_d(option = option, end = 0.9) +
+   scale_color_viridis_d(option = option, end = 0.9) +
+   labs(x = "Response burden", y = "Response rate [%]",
+        col = "Recruitment, incentive", fill = "Recruitment, incentive") +
+   Heimisc::my_theme() +
+   Heimisc::add_grid() +
+   theme(legend.position = "bottom")
\end{verbatim}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-9-1} 
\end{knitrout}

Despite drawing a line through such a point cloud is maybe a little critical and the sparsity of surveys with high response burdens (the ones we have are essentially outliers and influence the curve dramatically!), there is some indication, that any survey beyond 2000 points is just to burdensome for respondents. However, if they were recruited (i.e. agreed to participate) it looks different: Interestingly, there, the incentive seems absolutely essential. Further, the incentive flattens the curve - it will be interesting to compare the slopes of the incentive groups, once we have more data for surveys without a recruitment but with an incentive. However, anecdotal evidence from the TimeUse+ study suggests, that the incentive was important (see pre-test Caro; TRB24?).



\bibliographystyle{plain}
% \bibliography{grid}

\end{document}
